<!doctype html>
<html lang="zh-CN">
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
    <meta name="referrer" content="no-referrer-when-downgrade">
    

    <title>Llama Factory | 记录每个瞬间</title>
    <meta property="og:title" content="Llama Factory - 记录每个瞬间">
    <meta property="og:type" content="article">
        
    <meta property="article:published_time" content='2024-12-24T22:08:07&#43;08:00'>
        
        
    <meta property="article:modified_time" content='2024-12-24T22:08:07&#43;08:00'>
        
    <meta name="Keywords" content="">
    <meta name="description" content="Llama Factory">
        <meta name="author" content="隔壁老王">
        
    <meta property="og:url" content="https://code0xff.org/post/2024/12/llamafactory/">
    <link rel="shortcut icon" href='/favicon.ico'  type="image/x-icon">

    <link rel="stylesheet" href='/css/normalize.css'>
    <link rel="stylesheet" href='/css/style.css'>
    <script type="text/javascript" src="//cdn.bootcdn.net/ajax/libs/jquery/3.4.1/jquery.min.js"></script>

    
    
    
        <link href="https://cdn.bootcdn.net/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" rel="stylesheet">
    
    
    
    
</head>


<body>
    <header id="header" class="clearfix">
    <div class="container">
        <div class="col-group">
            <div class="site-name ">
                
                    <a id="logo" href="https://code0xff.org/">
                        记录每个瞬间
                    </a>
                
                
            </div>
            <div>
                <nav id="nav-menu" class="clearfix">
                    <a class="current" href="https://code0xff.org/">首页</a>
                    
                    <a  href="https://code0xff.org/linked/" title="链接">链接</a>
                    
                    <a  href="https://code0xff.org/archives/" title="归档">归档</a>
                    
                    <a  href="https://code0xff.org/about/" title="关于">关于</a>
                    
                </nav>
            </div>
        </div>
    </div>
</header>

    <div id="body">
        <div class="container">
            <div class="col-group">

                <div class="col-8" id="main">
                    
<div class="res-cons">
    
    <article class="post">
        <header>
            <h1 class="post-title">Llama Factory</h1>
        </header>
        <date class="post-meta meta-date">
            2024年12月24日
        </date>
<nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#安装">安装</a></li>
        <li><a href="#微调">微调</a></li>
        <li><a href="#详细分析">详细分析</a>
          <ul>
            <li><a href="#界面">界面</a></li>
            <li><a href="#指令格式">指令格式</a></li>
            <li><a href="#微调三种模式">微调三种模式</a></li>
            <li><a href="#几种方法">几种方法</a></li>
            <li><a href="#lora">LoRA</a></li>
            <li><a href="#评估">评估</a></li>
          </ul>
        </li>
        <li><a href="#参考">参考</a></li>
      </ul>
    </li>
  </ul>
</nav>
        
        <div class="post-meta">
            
            <span class="post-meta meta-tags">
                <ul class="clearfix">
                    <a href='/categories/%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90'>原理分析</a>
                </ul>
            </span>
            
        </div>
        
        
        <div class="post-meta">
            <span id="busuanzi_container_page_pv">|<span id="busuanzi_value_page_pv"></span><span>
                    阅读</span></span>
        </div>
        
        
        <div class="post-content">
            <h2 id="安装">安装</h2>
<p>如果是物理机安装，需要</p>
<ul>
<li>安装 GCC</li>
<li>安装 cuda</li>
<li>安装显卡驱动</li>
</ul>
<p>推荐容器安装</p>
<ul>
<li>也需要先安装 显卡驱动</li>
<li>PyTorch Release 24.02，nvidia 提供的 pytorch 镜像包，22G</li>
<li>包含 nvidia 相关的工具</li>
<li><a href="https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/rel-24-02.html">镜像包内容详情介绍</a></li>
</ul>
<p>docker compose 启动</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">cd</span> docker/docker-cuda/
</span></span><span style="display:flex;"><span>docker compose up -d
</span></span><span style="display:flex;"><span>docker compose <span style="color:#8be9fd;font-style:italic">exec</span> llamafactory bash
</span></span></code></pre></td></tr></table>
</div>
</div><p>支持 三种类型的 显卡</p>
<ul>
<li>nvidia 的</li>
<li>amd 的</li>
<li>华为 的</li>
</ul>
<p>执行</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>docker compose up -d
</span></span></code></pre></td></tr></table>
</div>
</div><p>这步时也会下载一些东西，需要修改下载的源配置<br>
docker file修改，修改成这个地址</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-dockerfile" data-lang="dockerfile"><span style="display:flex;"><span><span style="color:#ff79c6">ARG</span> <span style="color:#8be9fd;font-style:italic">PIP_INDEX</span><span style="color:#ff79c6">=</span>https://pypi.tuna.tsinghua.edu.cn/simple
</span></span></code></pre></td></tr></table>
</div>
</div><p>docker-compose yaml 修改：</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#ff79c6">services</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#ff79c6">llamafactory</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">build</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#ff79c6">dockerfile</span>: ./docker/docker-cuda/Dockerfile
</span></span><span style="display:flex;"><span>      <span style="color:#ff79c6">context</span>: ../..
</span></span><span style="display:flex;"><span>      <span style="color:#ff79c6">args</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#ff79c6">INSTALL_BNB</span>: <span style="color:#ff79c6">false</span>
</span></span><span style="display:flex;"><span>        <span style="color:#ff79c6">INSTALL_VLLM</span>: <span style="color:#ff79c6">false</span>
</span></span><span style="display:flex;"><span>        <span style="color:#ff79c6">INSTALL_DEEPSPEED</span>: <span style="color:#ff79c6">false</span>
</span></span><span style="display:flex;"><span>        <span style="color:#ff79c6">INSTALL_FLASHATTN</span>: <span style="color:#ff79c6">false</span>
</span></span><span style="display:flex;"><span>        <span style="color:#ff79c6">INSTALL_LIGER_KERNEL</span>: <span style="color:#ff79c6">false</span>
</span></span><span style="display:flex;"><span>        <span style="color:#ff79c6">INSTALL_HQQ</span>: <span style="color:#ff79c6">false</span>
</span></span><span style="display:flex;"><span>        <span style="color:#ff79c6">INSTALL_EETQ</span>: <span style="color:#ff79c6">false</span>
</span></span><span style="display:flex;"><span>        <span style="color:#ff79c6">PIP_INDEX</span>: https://pypi.tuna.tsinghua.edu.cn/simple
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">container_name</span>: llamafactory
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">volumes</span>:
</span></span><span style="display:flex;"><span>      - ../../hf_cache:/root/.cache/huggingface
</span></span><span style="display:flex;"><span>      - ../../ms_cache:/root/.cache/modelscope
</span></span><span style="display:flex;"><span>      - ../../om_cache:/root/.cache/openmind
</span></span><span style="display:flex;"><span>      - ../../data:/app/data
</span></span><span style="display:flex;"><span>      - ../../output:/app/output
</span></span><span style="display:flex;"><span>      - /data2/models:/data
</span></span><span style="display:flex;"><span>。。。
</span></span><span style="display:flex;"><span>。。。
</span></span></code></pre></td></tr></table>
</div>
</div><p>如上，修改了 PIP_INDEX，增加了一个 volumes，/data2</p>
<p>进入容器后，执行下面命令，启动 web ui</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>llamafactory-cli webui
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="微调">微调</h2>
<p>命令行启动</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>llamafactory-cli train <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>    --stage sft <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>    --do_train True <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>    --model_name_or_path /data/Qwen2.5-0.5B-Instruct <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>    --preprocessing_num_workers <span style="color:#bd93f9">16</span> <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>    --finetuning_type lora <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>    --template qwen <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>    --flash_attn auto <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>    --dataset_dir data <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>    --dataset alpaca_zh_demo <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>    --cutoff_len <span style="color:#bd93f9">2048</span> <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>    --learning_rate 5e-05 <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>    --num_train_epochs 3.0 <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>    --max_samples <span style="color:#bd93f9">100000</span> <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>    --per_device_train_batch_size <span style="color:#bd93f9">2</span> <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>    --gradient_accumulation_steps <span style="color:#bd93f9">8</span> <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>    --lr_scheduler_type cosine <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>    --max_grad_norm 1.0 <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>    --logging_steps <span style="color:#bd93f9">5</span> <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>    --save_steps <span style="color:#bd93f9">100</span> <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>    --warmup_steps <span style="color:#bd93f9">0</span> <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>    --packing False <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>    --report_to none <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>    --output_dir saves/Qwen2.5-0.5B-Instruct/lora/train_2024-12-16-05-58-02 <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>    --bf16 True <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>    --plot_loss True <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>    --ddp_timeout <span style="color:#bd93f9">180000000</span> <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>    --optim adamw_torch <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>    --lora_rank <span style="color:#bd93f9">8</span> <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>    --lora_alpha <span style="color:#bd93f9">16</span> <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>    --lora_dropout <span style="color:#bd93f9">0</span> <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>    --lora_target all
</span></span></code></pre></td></tr></table>
</div>
</div><p>自定义数据集的</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>llamafactory-cli train <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>     --stage sft <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>     --do_train True <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>     --model_name_or_path /data/Qwen2.5-0.5B-Instruct <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>     --preprocessing_num_workers <span style="color:#bd93f9">16</span> <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>     --finetuning_type lora <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>     --template qwen <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>     --flash_attn auto <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>     --dataset_dir /app/saves/data/Qwen2.5-0.5B-Instruct/2024-12-23_10_10_51 <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>     --dataset cyber_tuning <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>     --cutoff_len <span style="color:#bd93f9">2048</span> <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>     --learning_rate 5e-05 <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>     --num_train_epochs 3.0 <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>     --max_samples <span style="color:#bd93f9">100000</span> <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>     --per_device_train_batch_size <span style="color:#bd93f9">2</span> <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>     --gradient_accumulation_steps <span style="color:#bd93f9">8</span> <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>     --lr_scheduler_type cosine <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>     --max_grad_norm 1.0 <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>     --logging_steps <span style="color:#bd93f9">5</span> <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>     --save_steps <span style="color:#bd93f9">100</span> <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>     --warmup_steps <span style="color:#bd93f9">0</span> <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>     --packing False <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>     --report_to none <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>     --output_dir /app/saves/data/Qwen2.5-0.5B-Instruct/2024-12-23_10_10_51 <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>     --bf16 True <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>     --plot_loss True <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>     --ddp_timeout <span style="color:#bd93f9">180000000</span> <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>     --optim adamw_torch <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>     --lora_rank <span style="color:#bd93f9">8</span> <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>     --lora_alpha <span style="color:#bd93f9">16</span> <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>     --lora_dropout <span style="color:#bd93f9">0</span> <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>     --lora_target all <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>	 --lora_alpha <span style="color:#bd93f9">80</span> <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>     --lora_rank <span style="color:#bd93f9">10</span> <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c"></span>	 --use_rslora True
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="详细分析">详细分析</h2>
<h3 id="界面">界面</h3>
<p>
        <a data-fancybox="gallery" href="https://v1.ax1x.com/2025/01/06/7VdW8H.png">
            <img class="mx-auto" alt="" src="https://v1.ax1x.com/2025/01/06/7VdW8H.png" />
        </a>
    <br>

        <a data-fancybox="gallery" href="https://v1.ax1x.com/2025/01/06/7Vd5j9.png">
            <img class="mx-auto" alt="" src="https://v1.ax1x.com/2025/01/06/7Vd5j9.png" />
        </a>
    <br>

        <a data-fancybox="gallery" href="https://v1.ax1x.com/2025/01/06/7VdVmY.png">
            <img class="mx-auto" alt="" src="https://v1.ax1x.com/2025/01/06/7VdVmY.png" />
        </a>
    </p>
<h3 id="指令格式">指令格式</h3>
<p>两种指令格式</p>
<ul>
<li>Alpaca</li>
<li>ShareGPT</li>
</ul>
<p>对比</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th><strong>Alpaca Format</strong></th>
<th><strong>ShareGPT Format</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Purpose</strong></td>
<td>Instruction-following</td>
<td>Conversational AI</td>
</tr>
<tr>
<td><strong>Structure</strong></td>
<td>Instruction, Input (optional), Output</td>
<td>Multi-turn dialogue (user-assistant messages)</td>
</tr>
<tr>
<td><strong>Flexibility</strong></td>
<td>Handles single-turn, structured tasks</td>
<td>Handles multi-turn, natural conversations</td>
</tr>
<tr>
<td><strong>Example Use Case</strong></td>
<td>Chatbot following commands (e.g., write a story)</td>
<td>Multi-turn Q&amp;A chatbot (e.g., customer support)</td>
</tr>
</tbody>
</table>
<p>Alpaca Format 例子</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>[
</span></span><span style="display:flex;"><span>  {
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">&#34;instruction&#34;</span>: <span style="color:#f1fa8c">&#34;识别并解释给定列表中的两个科学理论：细胞理论和日心说。&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">&#34;input&#34;</span>: <span style="color:#f1fa8c">&#34;&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">&#34;output&#34;</span>: <span style="color:#f1fa8c">&#34;细胞理论是生物科学的一个理论，它认为所有生命体都是由微小的基本单元——细胞所构成。这是生物学的一个基础理论，认为&gt;细胞是所有生物的基本结构和功能单位，所有的生物都是由一个或多个细胞组成，细胞只能通过细胞分裂产生新的细胞。这一理论由薛定谔、施
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">瓦内和雪莱于1839年首次提出。\n\n日心说是指太阳是太阳系的中心，也就是说，行星围绕太阳旋转的理论。这个理论打破了传统的地心说观点
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">，认为地球并不是宇宙的中心。日心说的提出者是尼古拉·哥白尼，他在16世纪初发表了他的著作《天体运行论》，阐述了太阳系行星围绕太阳&gt;运行的模型，为天文学的发展做出了巨大贡献。&#34;</span>
</span></span><span style="display:flex;"><span>  },
</span></span><span style="display:flex;"><span>  {
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">&#34;instruction&#34;</span>: <span style="color:#f1fa8c">&#34;输入三支篮球队的名称并生成一个适当的口号。&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">&#34;input&#34;</span>: <span style="color:#f1fa8c">&#34;输入：俄克拉荷马城雷霆队，芝加哥公牛队，布鲁克林网队。&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">&#34;output&#34;</span>: <span style="color:#f1fa8c">&#34;输出： \&#34;雷霆，公牛和网队：各显神通，角逐群雄!\&#34;&#34;</span>
</span></span><span style="display:flex;"><span>  },
</span></span><span style="display:flex;"><span>  {
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">&#34;instruction&#34;</span>: <span style="color:#f1fa8c">&#34;比较以下两位著名人物，使用特征列表并以句子形式提供输出。&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">&#34;input&#34;</span>: <span style="color:#f1fa8c">&#34;人物1：阿尔伯特·爱因斯坦\n人物2：斯蒂芬·霍金\n特征：智力、教育、对世界的影响&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">&#34;output&#34;</span>: <span style="color:#f1fa8c">&#34;阿尔伯特·爱因斯坦和斯蒂芬·霍金都是拥有极其出色智力的人物。两人都取得过非常高的教育成就，他们推进了科学发展并在&gt;世界范围内产生了深远的影响。爱因斯坦以其相对论和质能关系公式而闻名，而霍金以其关于黑洞和宇宙的发现而著称。两位科学家都以其深厚
</span></span></span><span style="display:flex;"><span><span style="color:#f1fa8c">的学识和非凡的贡献影响了世界。&#34;</span>
</span></span><span style="display:flex;"><span>  },
</span></span><span style="display:flex;"><span>  {
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">&#34;instruction&#34;</span>: <span style="color:#f1fa8c">&#34;请根据以下文章构建一个需要回答的问题。&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">&#34;input&#34;</span>: <span style="color:#f1fa8c">&#34;&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">&#34;output&#34;</span>: <span style="color:#f1fa8c">&#34;我不好意思，您没有提供任何信息或文章来供我根据它构建问题。 请提供相应的文章或信息，以便我能够帮助您构建问题。&#34;</span>
</span></span><span style="display:flex;"><span>  },
</span></span><span style="display:flex;"><span>  {
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">&#34;instruction&#34;</span>: <span style="color:#f1fa8c">&#34;写一个关于上周末你做了什么的简短叙述。&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">&#34;input&#34;</span>: <span style="color:#f1fa8c">&#34;&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">&#34;output&#34;</span>: <span style="color:#f1fa8c">&#34;由于我是一个人工智能助手，所以我并不能真正的“做”什么事情。我的“工作”时间是24/7无休，确保我随时可供用户提供帮助&gt;和服务。所以在上周末，我一直都在“工作”，在响应用户的指令，帮助他们查询信息，执行操作，解决问题。&#34;</span>
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>]
</span></span></code></pre></td></tr></table>
</div>
</div><p>ShareGPT Format 例子</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">19
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">20
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">21
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">22
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">23
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">24
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">25
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">26
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">27
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">28
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">29
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">30
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">31
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">32
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">33
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">34
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">35
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">36
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">37
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">38
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">39
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-json" data-lang="json"><span style="display:flex;"><span>[
</span></span><span style="display:flex;"><span>  {
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">&#34;conversations&#34;</span>: [
</span></span><span style="display:flex;"><span>      {
</span></span><span style="display:flex;"><span>        <span style="color:#ff79c6">&#34;from&#34;</span>: <span style="color:#f1fa8c">&#34;human&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ff79c6">&#34;value&#34;</span>: <span style="color:#f1fa8c">&#34;Hi, I have some ingredients and I want to cook something. Can you help me find a recipe?&#34;</span>
</span></span><span style="display:flex;"><span>      },
</span></span><span style="display:flex;"><span>      {
</span></span><span style="display:flex;"><span>        <span style="color:#ff79c6">&#34;from&#34;</span>: <span style="color:#f1fa8c">&#34;gpt&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ff79c6">&#34;value&#34;</span>: <span style="color:#f1fa8c">&#34;Of course! I can help you with that. Please tell me what ingredients you have.&#34;</span>
</span></span><span style="display:flex;"><span>      },
</span></span><span style="display:flex;"><span>      {
</span></span><span style="display:flex;"><span>        <span style="color:#ff79c6">&#34;from&#34;</span>: <span style="color:#f1fa8c">&#34;human&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ff79c6">&#34;value&#34;</span>: <span style="color:#f1fa8c">&#34;I have chicken, bell peppers, and rice.&#34;</span>
</span></span><span style="display:flex;"><span>      },
</span></span><span style="display:flex;"><span>      {
</span></span><span style="display:flex;"><span>        <span style="color:#ff79c6">&#34;from&#34;</span>: <span style="color:#f1fa8c">&#34;function_call&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ff79c6">&#34;value&#34;</span>: <span style="color:#f1fa8c">&#34;{\&#34;name\&#34;: \&#34;search_recipes\&#34;, \&#34;arguments\&#34;: {\&#34;ingredients\&#34;: [\&#34;chicken\&#34;, \&#34;bell peppers\&#34;, \&#34;rice\&#34;]}}&#34;</span>
</span></span><span style="display:flex;"><span>      },
</span></span><span style="display:flex;"><span>      {
</span></span><span style="display:flex;"><span>        <span style="color:#ff79c6">&#34;from&#34;</span>: <span style="color:#f1fa8c">&#34;observation&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ff79c6">&#34;value&#34;</span>: <span style="color:#f1fa8c">&#34;{\&#34;recipes\&#34;: [{\&#34;name\&#34;: \&#34;Chicken and Bell Pepper Stir Fry\&#34;, \&#34;ingredients\&#34;: [\&#34;chicken\&#34;, \&#34;bell peppers\&#34;, \&#34;rice\&#34;], \&#34;instructions\&#34;: \&#34;Cut the chicken into small pieces. Slice the bell peppers. Cook the rice. Stir fry the chicken and bell peppers. Serve over rice.\&#34;}, {\&#34;name\&#34;: \&#34;Chicken and Rice Casserole\&#34;, \&#34;ingredients\&#34;: [\&#34;chicken\&#34;, \&#34;bell peppers\&#34;, \&#34;rice\&#34;], \&#34;instructions\&#34;: \&#34;Cook the chicken and rice separately. Mix them together with the bell peppers in a casserole dish. Bake until golden brown.\&#34;}]}&#34;</span>
</span></span><span style="display:flex;"><span>      },
</span></span><span style="display:flex;"><span>      {
</span></span><span style="display:flex;"><span>        <span style="color:#ff79c6">&#34;from&#34;</span>: <span style="color:#f1fa8c">&#34;gpt&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ff79c6">&#34;value&#34;</span>: <span style="color:#f1fa8c">&#34;I found two recipes for you. The first one is \&#34;Chicken and Bell Pepper Stir Fry\&#34;. The instructions are: Cut the chicken into small pieces. Slice the bell peppers. Cook the rice. Stir fry the chicken and bell peppers. Serve over rice. The second one is \&#34;Chicken and Rice Casserole\&#34;. The instructions are: Cook the chicken and rice separately. Mix them together with the bell peppers in a casserole dish. Bake until golden brown. Which one would you like to try?&#34;</span>
</span></span><span style="display:flex;"><span>      },
</span></span><span style="display:flex;"><span>      {
</span></span><span style="display:flex;"><span>        <span style="color:#ff79c6">&#34;from&#34;</span>: <span style="color:#f1fa8c">&#34;human&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ff79c6">&#34;value&#34;</span>: <span style="color:#f1fa8c">&#34;They both sound delicious, but I think I&#39;ll try the stir fry. Can you order the ingredients for me?&#34;</span>
</span></span><span style="display:flex;"><span>      },
</span></span><span style="display:flex;"><span>      {
</span></span><span style="display:flex;"><span>        <span style="color:#ff79c6">&#34;from&#34;</span>: <span style="color:#f1fa8c">&#34;gpt&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#ff79c6">&#34;value&#34;</span>: <span style="color:#f1fa8c">&#34;I&#39;m sorry, but as an AI, I don&#39;t have the capability to perform external tasks such as ordering ingredients. However, I can help you find more recipes or provide cooking tips if you need.&#34;</span>
</span></span><span style="display:flex;"><span>      }
</span></span><span style="display:flex;"><span>    ],
</span></span><span style="display:flex;"><span>    <span style="color:#ff79c6">&#34;tools&#34;</span>: <span style="color:#f1fa8c">&#34;[{\&#34;name\&#34;: \&#34;search_recipes\&#34;, \&#34;description\&#34;: \&#34;Search for recipes based on ingredients\&#34;, \&#34;parameters\&#34;: {\&#34;type\&#34;: \&#34;object\&#34;, \&#34;properties\&#34;: {\&#34;ingredients\&#34;: {\&#34;type\&#34;: \&#34;array\&#34;, \&#34;items\&#34;: {\&#34;type\&#34;: \&#34;string\&#34;}, \&#34;description\&#34;: \&#34;The ingredients to search for\&#34;}}, \&#34;required\&#34;: [\&#34;ingredients\&#34;]}}]&#34;</span>
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>]
</span></span></code></pre></td></tr></table>
</div>
</div><h3 id="微调三种模式">微调三种模式</h3>
<table>
<thead>
<tr>
<th><strong>Aspect</strong></th>
<th><strong>LoRA (Low-Rank Adaptation)</strong></th>
<th><strong>Freeze</strong></th>
<th><strong>Full Fine-Tuning</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>How It Works</strong></td>
<td>Adds small trainable <strong>low-rank matrices</strong> to specific layers (e.g., attention heads) while keeping the base model frozen.</td>
<td>Freezes most of the model&rsquo;s layers and only fine-tunes specific layers (e.g., final layer or task-specific heads).</td>
<td>Updates all the parameters of the model, training the entire model for the new task.</td>
</tr>
<tr>
<td><strong>Trainable Parameters</strong></td>
<td>Introduces new parameters (low-rank matrices), significantly reducing the number of trainable parameters.</td>
<td>Trains only the selected layers, while the rest of the model remains unchanged.</td>
<td>All parameters are trainable, requiring large computational resources.</td>
</tr>
<tr>
<td><strong>Memory/Compute Usage</strong></td>
<td>Very efficient; requires far less memory and compute compared to full fine-tuning.</td>
<td>Moderate efficiency; requires less memory than full fine-tuning but more than LoRA.</td>
<td>High memory and compute usage, as the entire model is updated.</td>
</tr>
<tr>
<td><strong>Base Model Dependency</strong></td>
<td>The base model remains fully intact; LoRA layers can be added or removed modularly.</td>
<td>The base model is partially frozen, and updates are made to selected layers.</td>
<td>The base model is entirely modified, making it task-specific.</td>
</tr>
<tr>
<td><strong>Modularity</strong></td>
<td>Highly modular; you can share LoRA weights separately without altering the base model.</td>
<td>Less modular than LoRA; trained layers are not as easily reusable across tasks.</td>
<td>Not modular; the fine-tuned model is specific to the task.</td>
</tr>
<tr>
<td><strong>Flexibility</strong></td>
<td>Designed for specific parts of the model (e.g., attention layers), limiting flexibility.</td>
<td>Highly flexible; any layer or combination of layers can be chosen for fine-tuning.</td>
<td>Fully flexible; every parameter is trainable.</td>
</tr>
<tr>
<td><strong>Fine-Tuning Speed</strong></td>
<td>Very fast; only a small number of parameters are optimized.</td>
<td>Moderate speed; faster than full fine-tuning but slower than LoRA.</td>
<td>Slow; training the entire model requires significant time.</td>
</tr>
<tr>
<td><strong>Storage Requirements</strong></td>
<td>Stores only the additional LoRA parameters, which are very small in size.</td>
<td>Saves the modified layers; smaller storage requirements than full fine-tuning.</td>
<td>Requires storing the entire updated model, consuming significant storage.</td>
</tr>
<tr>
<td><strong>Resource Efficiency</strong></td>
<td>Extremely efficient for large models (e.g., LLaMA, GPT); can work with limited hardware.</td>
<td>Less efficient than LoRA but more efficient than full fine-tuning.</td>
<td>Requires substantial hardware and resources.</td>
</tr>
<tr>
<td><strong>Pretrained Knowledge Preservation</strong></td>
<td>Excellent; the base model is untouched, so pre-trained knowledge is preserved.</td>
<td>Good; frozen layers retain pre-trained knowledge, while tuned layers adapt to the new task.</td>
<td>Risk of overfitting or catastrophic forgetting of pre-trained knowledge.</td>
</tr>
<tr>
<td><strong>Use Case Examples</strong></td>
<td>Fine-tuning very large models on specific tasks with limited resources.</td>
<td>Adapting the model to new tasks by tuning only a few task-relevant layers.</td>
<td>Training the model for a completely new domain or task with ample resources.</td>
</tr>
<tr>
<td><strong>Common Scenarios</strong></td>
<td>Fine-tuning large LLMs like GPT, LLaMA, or BERT on specific downstream tasks.</td>
<td>Updating final layers for classification or regression tasks with pre-trained features.</td>
<td>Building a domain-specific model from scratch using pre-trained weights.</td>
</tr>
<tr>
<td><strong>Key Advantages</strong></td>
<td>- Highly efficient. <!-- raw HTML omitted --> - Small storage overhead. <!-- raw HTML omitted --> - Easy to switch tasks by loading specific LoRA weights.</td>
<td>- Flexible in selecting which layers to train. <!-- raw HTML omitted --> - Retains much of the pre-trained knowledge.</td>
<td>- Full control over the model’s behavior. <!-- raw HTML omitted --> - Best for completely transforming the model.</td>
</tr>
<tr>
<td><strong>Key Disadvantages</strong></td>
<td>- Limited to parts of the model where LoRA is implemented. <!-- raw HTML omitted --> - May not be sufficient for complex adaptations.</td>
<td>- Requires careful selection of layers to tune. <!-- raw HTML omitted --> - May not fully adapt the model to challenging tasks.</td>
<td>- Expensive in terms of memory, compute, and storage. <!-- raw HTML omitted --> - Risk of overfitting.</td>
</tr>
</tbody>
</table>
<h3 id="几种方法">几种方法</h3>
<p>量化等级 (Quantization Level)<br>
Quantization level refers to the precision of numerical representations used during training or inference. Lower precision reduces model size and computation requirements.</p>
<ul>
<li>
<p><strong>Options</strong>:</p>
<ul>
<li><code>none</code>: No quantization is applied; the model uses full precision (e.g., FP32 or FP16). This results in the highest accuracy but requires more memory and compute.</li>
<li><code>4-bit/8-bit</code>: Reduces numerical precision to 4-bit or 8-bit integers or floats, significantly lowering memory and compute usage while sacrificing a small amount of accuracy.</li>
</ul>
</li>
<li>
<p><strong>Purpose</strong>:</p>
<ul>
<li>Reduce memory and computational requirements for large models.</li>
<li>Enable deployment on resource-constrained hardware like GPUs with limited memory.</li>
</ul>
</li>
</ul>
<hr>
<p>量化方法 (Quantization Method)<br>
Quantization methods define how the quantization process is applied to the model&rsquo;s weights or activations.</p>
<ul>
<li>
<p><strong>Options</strong>:</p>
<ul>
<li><code>bitsandbytes</code>:
<ul>
<li>A widely-used library for efficient quantization.</li>
<li>Supports 4-bit and 8-bit quantization for large models with negligible performance degradation.</li>
<li>Commonly used with LLMs like GPT and LLaMA.</li>
</ul>
</li>
<li><code>hqq</code>:
<ul>
<li>A hypothetical or custom quantization algorithm (specific to your framework).</li>
<li>Could have specific optimizations for performance or compatibility.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Purpose</strong>:</p>
<ul>
<li>Reduce model size during training or inference.</li>
<li>Allow for high-speed computations on hardware with limited precision support (e.g., GPUs with Tensor Cores).</li>
</ul>
</li>
</ul>
<hr>
<p>RoPE插值方法 (RoPE Interpolation Method)<br>
RoPE (Rotary Position Embedding) is a technique used in transformer models to encode positional information.</p>
<ul>
<li>
<p><strong>Options</strong>:</p>
<ul>
<li><code>none</code>:
<ul>
<li>No interpolation applied; uses standard rotary embeddings as defined during training.</li>
<li>Suitable for fixed-length input sequences.</li>
</ul>
</li>
<li><code>linear</code>:
<ul>
<li>Applies linear interpolation to extend the range of rotary embeddings.</li>
<li>Useful for models trained on shorter sequences but used for longer sequences during inference.</li>
</ul>
</li>
<li><code>dynamic</code>:
<ul>
<li>Dynamically adjusts rotary embeddings based on the sequence length or other factors.</li>
<li>Provides more flexibility but may introduce overhead.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Purpose</strong>:</p>
<ul>
<li>Enhance the model’s ability to process sequences of varying lengths.</li>
<li>Adapt rotary embeddings to new contexts or longer sequences.</li>
</ul>
</li>
</ul>
<hr>
<p>加速方法 (Acceleration Method)<br>
Acceleration methods refer to optimizations used to speed up model inference or training by leveraging efficient algorithms or hardware-specific features.</p>
<ul>
<li>
<p><strong>Options</strong>:</p>
<ul>
<li><code>auto</code>:
<ul>
<li>Automatically selects the best acceleration method based on the hardware and framework configuration.</li>
</ul>
</li>
<li><code>flashattn2</code>:
<ul>
<li>Refers to <strong>Flash Attention 2</strong>, an optimized attention mechanism that improves speed and memory usage for transformer models.</li>
<li>Ideal for large-scale models and long sequences.</li>
</ul>
</li>
<li><code>unsloth</code>:
<ul>
<li>Likely a custom or experimental acceleration method (specific to your framework).</li>
<li>May focus on balancing memory efficiency and computation speed.</li>
</ul>
</li>
<li><code>liger_kernel</code>:
<ul>
<li>A custom kernel optimized for specific hardware or computation patterns.</li>
<li>Could provide tailored optimizations for matrix multiplications or attention mechanisms.</li>
</ul>
</li>
</ul>
</li>
<li>
<p><strong>Purpose</strong>:</p>
<ul>
<li>Improve training and inference speed.</li>
<li>Reduce memory consumption without affecting accuracy.</li>
</ul>
</li>
</ul>
<p>几种方法总结</p>
<table>
<thead>
<tr>
<th><strong>Feature</strong></th>
<th><strong>Purpose</strong></th>
<th><strong>Key Options</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>量化等级</strong></td>
<td>Control model precision and resource usage.</td>
<td><code>none</code>, <code>4-bit</code>, <code>8-bit</code></td>
</tr>
<tr>
<td><strong>量化方法</strong></td>
<td>Define the quantization algorithm.</td>
<td><code>bitsandbytes</code>, <code>hqq</code></td>
</tr>
<tr>
<td><strong>RoPE插值方法</strong></td>
<td>Adjust rotary embeddings for flexibility.</td>
<td><code>none</code>, <code>linear</code>, <code>dynamic</code></td>
</tr>
<tr>
<td><strong>加速方法</strong></td>
<td>Optimize training/inference for speed.</td>
<td><code>auto</code>, <code>flashattn2</code>, <code>unsloth</code>, <code>liger_kernel</code></td>
</tr>
</tbody>
</table>
<h3 id="lora">LoRA</h3>
<p><strong>LoRA 秩 (LoRA Rank)</strong></p>
<ul>
<li><strong>Meaning</strong>: Determines the rank of the low-rank decomposition matrices used in LoRA.</li>
<li><strong>Impact</strong>:
<ul>
<li>A higher rank increases the capacity of LoRA, allowing it to learn more complex patterns but uses more memory.</li>
<li>A lower rank makes the model more lightweight but might limit its adaptability.</li>
</ul>
</li>
<li><strong>Default</strong>: Often set between 4 and 16 depending on the model and hardware.</li>
</ul>
<p><strong>LoRA 缩放系数 (LoRA Scaling Factor)</strong></p>
<ul>
<li><strong>Meaning</strong>: A scaling factor applied to the LoRA parameters to adjust their contribution to the model.</li>
<li><strong>Impact</strong>:
<ul>
<li>Higher scaling factors emphasize LoRA&rsquo;s contribution.</li>
<li>Lower scaling factors blend LoRA more subtly with the base model.</li>
</ul>
</li>
</ul>
<p><strong>LoRA 随机丢弃 (LoRA Dropout)</strong></p>
<ul>
<li><strong>Meaning</strong>: Adds dropout regularization to the LoRA weights during training.</li>
<li><strong>Purpose</strong>:
<ul>
<li>Helps prevent overfitting.</li>
<li>Improves generalization of the fine-tuned model.</li>
</ul>
</li>
<li><strong>Range</strong>: Typically between <code>0</code> (no dropout) and <code>0.5</code>.</li>
</ul>
<p><strong>LoRA+ 学习率比例 (LoRA+ Learning Rate Scale)</strong></p>
<ul>
<li><strong>Meaning</strong>: Scales the learning rate applied to the LoRA parameters.</li>
<li><strong>Purpose</strong>:
<ul>
<li>Fine-tune LoRA parameters more efficiently by adjusting their learning rate.</li>
</ul>
</li>
<li><strong>Impact</strong>:
<ul>
<li>Higher values speed up learning but risk instability.</li>
<li>Lower values ensure stability at the cost of slower training.</li>
</ul>
</li>
</ul>
<p><strong>新建适配器 (New Adapter)</strong></p>
<ul>
<li><strong>Meaning</strong>: Creates a new, randomly initialized adapter configuration on top of the existing one.</li>
<li><strong>Purpose</strong>:
<ul>
<li>Useful for training different configurations for multiple tasks without overwriting existing ones.</li>
</ul>
</li>
</ul>
<p><strong>使用 rslora (Use rslora)</strong></p>
<ul>
<li><strong>Meaning</strong>: Enables a &ldquo;stable LoRA&rdquo; method that adds regularization for better training convergence.</li>
<li><strong>Purpose</strong>:
<ul>
<li>Prevents LoRA parameters from diverging significantly during training.</li>
</ul>
</li>
</ul>
<p><strong>使用 DoRA (Use DoRA)</strong></p>
<ul>
<li><strong>Meaning</strong>: Stands for <strong>Decomposed Residual Adapter</strong>, which provides a more granular decomposition in fine-tuning.</li>
<li><strong>Purpose</strong>:
<ul>
<li>Improves efficiency by focusing on specific residual connections in the model.</li>
</ul>
</li>
</ul>
<p><strong>使用 PiSSA 方法 (Use PiSSA)</strong></p>
<ul>
<li><strong>Meaning</strong>: PiSSA refers to a custom or experimental optimization technique for LoRA training.</li>
<li><strong>Purpose</strong>:
<ul>
<li>Adds advanced optimizations for parameter-efficient fine-tuning.</li>
</ul>
</li>
</ul>
<p><strong>LoRA 作用模块 (LoRA Target Modules)</strong></p>
<ul>
<li><strong>Meaning</strong>: Specifies the target modules (e.g., attention layers, feed-forward layers) where LoRA is applied.</li>
<li><strong>Purpose</strong>:
<ul>
<li>Fine-tune only specific parts of the model to save resources or improve performance on specific tasks.</li>
</ul>
</li>
</ul>
<p><strong>附加模块 (Additional Modules)</strong></p>
<ul>
<li><strong>Meaning</strong>: Specifies additional trainable modules outside of LoRA layers.</li>
<li><strong>Purpose</strong>:
<ul>
<li>Enables training of non-LoRA components, offering broader customization for specific tasks.</li>
</ul>
</li>
</ul>
<p>Summary Table</p>
<table>
<thead>
<tr>
<th><strong>Parameter</strong></th>
<th><strong>Purpose</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>LoRA 秩</strong></td>
<td>Sets the rank of the LoRA matrices, controlling model complexity and resource usage.</td>
</tr>
<tr>
<td><strong>LoRA 缩放系数</strong></td>
<td>Adjusts the scale of LoRA parameters to balance their influence in the model.</td>
</tr>
<tr>
<td><strong>LoRA 随机丢弃</strong></td>
<td>Introduces dropout for regularization to prevent overfitting.</td>
</tr>
<tr>
<td><strong>LoRA+ 学习率比例</strong></td>
<td>Scales the learning rate of LoRA parameters for balanced fine-tuning.</td>
</tr>
<tr>
<td><strong>新建适配器</strong></td>
<td>Creates a new adapter configuration for multi-domain or task-specific tuning.</td>
</tr>
<tr>
<td><strong>使用 rslora</strong></td>
<td>Ensures stable training of LoRA parameters with added regularization.</td>
</tr>
<tr>
<td><strong>使用 DoRA</strong></td>
<td>Adds decomposed residual adapters for more granular fine-tuning.</td>
</tr>
<tr>
<td><strong>使用 PiSSA</strong></td>
<td>Adds experimental methods for advanced LoRA optimization.</td>
</tr>
<tr>
<td><strong>LoRA 作用模块</strong></td>
<td>Defines the modules (e.g., attention layers) where LoRA will be applied.</td>
</tr>
<tr>
<td><strong>附加模块</strong></td>
<td>Specifies trainable modules beyond LoRA layers, enabling broader customization.</td>
</tr>
</tbody>
</table>
<h3 id="评估">评估</h3>
<p>命令</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>llamafactory-cli <span style="color:#8be9fd;font-style:italic">eval</span> qwen_lora_eval.yaml
</span></span></code></pre></td></tr></table>
</div>
</div><p>配置文件</p>
<div class="highlight"><div style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">
<table style="border-spacing:0;padding:0;margin:0;border:0;"><tr><td style="vertical-align:top;padding:0;margin:0;border:0;">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">14
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">15
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">16
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">17
</span><span style="white-space:pre;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">18
</span></code></pre></td>
<td style="vertical-align:top;padding:0;margin:0;border:0;;width:100%">
<pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#6272a4">### model</span>
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">model_name_or_path</span>: /data/Qwen2-1.5B-Chat
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">adapter_name_or_path</span>: /app/saves/data/Qwen2-1.5B-Chat/2024-12-25_02_02_19
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4">### method</span>
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">finetuning_type</span>: lora
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4">### dataset</span>
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">task: mmlu_test  # choices</span>: [mmlu_test, ceval_validation, cmmlu_test]
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">template</span>: fewshot
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">lang</span>: en
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">n_shot</span>: <span style="color:#bd93f9">5</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4">### output</span>
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">save_dir</span>: /app/saves/data/Qwen2-1.5B-Chat/eval/res
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4">### eval</span>
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">batch_size</span>: <span style="color:#bd93f9">4</span>
</span></span></code></pre></td></tr></table>
</div>
</div><h2 id="参考">参考</h2>
<ul>
<li><a href="https://github.com/hiyouga/LLaMA-Factory">GitHub</a></li>
<li><a href="https://llamafactory.readthedocs.io/zh-cn/latest/">文档</a></li>
<li><a href="https://www.datacamp.com/tutorial/llama-factory-web-ui-guide-fine-tuning-llms">LlaMA-Factory WebUI Beginner&rsquo;s Guide: Fine-Tuning LLMs</a></li>
<li><a href="https://cloud.baidu.com/article/3251091">Huggingface镜像站：加速你的AI之旅，体验国内高速下载</a></li>
<li><a href="https://www.53ai.com/news/qianyanjishu/2024070974390.html">微调神器LLaMA-Factory官方保姆级教程来了，从环境搭建到模型训练评估全覆盖</a></li>
<li><a href="https://baaidata.csdn.net/66e3ddc9e2ce0119e0a1bd35.html">LLaMA-Factory自定义数据集微调</a></li>
<li><a href="https://juejin.cn/post/7398046513810669608">LLaMA-Factory 大模型微调超简单，从零开始玩转大模型微调</a></li>
<li><a href="https://docs.nvidia.com/datacenter/dcgm/latest/gpu-telemetry/dcgm-exporter.html">dcgm-exporter</a></li>
<li><a href="https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/rel-24-02.html">NVIDIA Optimized Frameworks</a></li>
<li><a href="https://modelscope.cn/models/Qwen/Qwen2.5-0.5B-Instruct/files">Qwen2.5-0.5B-Instruct 模搭社区下载</a></li>
<li><a href="https://github.com/hiyouga/LLaMA-Factory/blob/main/data/dataset_info.json">data_set运行的数据集</a></li>
<li><a href="https://mp.weixin.qq.com/s/l9wbClRvelhG4nT5jS98xA">大模型高效训练一体框架 LLaMA Factory</a></li>
</ul>

			<script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>
        </div>

        


        


        <div class="post-meta meta-tags">
            
            <ul class="clearfix">
                
                <li><a href='/tags/llamafactory'>LlamaFactory</a></li>
                
            </ul>
            
        </div>
    </article>
    
    

    
    
    <div class="post bg-white">
      <script src="https://utteranc.es/client.js"
            repo= "himichael/hugoblogtalks"
            issue-term="pathname"
            theme="github-light"
            crossorigin="anonymous"
            async>
      </script>
    </div>
    
</div>


                </div>

                <div id="secondary">
    <section class="widget">
        <form id="search" action='https://code0xff.org/search/' method="get" accept-charset="utf-8" target="_blank" _lpchecked="1">
      
      <input type="text" name="q" maxlength="20" placeholder="Search">
      <input type="hidden" name="sitesearch" value="https://code0xff.org/">
      <button type="submit" class="submit icon-search"></button>
</form>
    </section>
    
    <section class="widget">
        <h3 class="widget-title">最近文章</h3>
<ul class="widget-list">
    
    <li>
        <a href="https://code0xff.org/post/2025/01/logs_collection_based_cloud/" title="云环境中的日志收集和处理方案">云环境中的日志收集和处理方案</a>
    </li>
    
    <li>
        <a href="https://code0xff.org/post/2025/01/btrace/" title="BTrace">BTrace</a>
    </li>
    
    <li>
        <a href="https://code0xff.org/post/2025/01/yarn/" title="YARN原理分析">YARN原理分析</a>
    </li>
    
    <li>
        <a href="https://code0xff.org/post/2025/01/spark_register_source/" title="Spark 注册数据源">Spark 注册数据源</a>
    </li>
    
    <li>
        <a href="https://code0xff.org/post/2025/01/spark_core_2/" title="Spark Core相关-2">Spark Core相关-2</a>
    </li>
    
    <li>
        <a href="https://code0xff.org/post/2024/12/llamafactory/" title="Llama Factory">Llama Factory</a>
    </li>
    
    <li>
        <a href="https://code0xff.org/post/2024/11/spark_core/" title="Spark Core相关-1">Spark Core相关-1</a>
    </li>
    
    <li>
        <a href="https://code0xff.org/post/2024/11/sae_tunnel/" title="sea tunnel">sea tunnel</a>
    </li>
    
    <li>
        <a href="https://code0xff.org/post/2024/11/k8s_networks/" title="k8s 网络">k8s 网络</a>
    </li>
    
    <li>
        <a href="https://code0xff.org/post/2024/11/ozone/" title="ozone">ozone</a>
    </li>
    
</ul>
    </section>

    

    <section class="widget">
        <h3 class="widget-title"><a href='/categories/'>分类</a></h3>
<ul class="widget-list">
    
    <li><a href="https://code0xff.org/categories/%E5%88%86%E5%B8%83%E5%BC%8F/">分布式 (6)</a></li>
    
    <li><a href="https://code0xff.org/categories/%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/">原理分析 (26)</a></li>
    
    <li><a href="https://code0xff.org/categories/%E5%95%86%E4%B8%9A/">商业 (3)</a></li>
    
    <li><a href="https://code0xff.org/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据 (56)</a></li>
    
    <li><a href="https://code0xff.org/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/">数据库 (41)</a></li>
    
    <li><a href="https://code0xff.org/categories/%E6%96%87%E5%AD%A6%E5%92%8C%E8%89%BA%E6%9C%AF/">文学和艺术 (3)</a></li>
    
    <li><a href="https://code0xff.org/categories/%E6%97%85%E8%A1%8C/">旅行 (12)</a></li>
    
    <li><a href="https://code0xff.org/categories/%E6%9E%B6%E6%9E%84/">架构 (14)</a></li>
    
    <li><a href="https://code0xff.org/categories/%E7%AE%97%E6%B3%95/">算法 (8)</a></li>
    
    <li><a href="https://code0xff.org/categories/%E7%B3%BB%E7%BB%9F/">系统 (7)</a></li>
    
    <li><a href="https://code0xff.org/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">编程语言 (7)</a></li>
    
    <li><a href="https://code0xff.org/categories/%E8%A1%8C%E4%B8%9A%E8%A7%82%E5%AF%9F/">行业观察 (3)</a></li>
    
    <li><a href="https://code0xff.org/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记 (14)</a></li>
    
    <li><a href="https://code0xff.org/categories/%E9%9A%8F%E4%BE%BF%E5%86%99%E5%86%99/">随便写写 (3)</a></li>
    
</ul>
    </section>

    <section class="widget">
        <h3 class="widget-title">归档</a></h3>
<ul class="widget-list">
    
    <li><a href="https://code0xff.org/years/2021%E5%B9%B4/">2021年 (41)</a></li>
    
    <li><a href="https://code0xff.org/years/2022%E5%B9%B4/">2022年 (55)</a></li>
    
    <li><a href="https://code0xff.org/years/2023%E5%B9%B4/">2023年 (54)</a></li>
    
    <li><a href="https://code0xff.org/years/2024%E5%B9%B4/">2024年 (48)</a></li>
    
    <li><a href="https://code0xff.org/years/2025%E5%B9%B4/">2025年 (5)</a></li>
    
</ul>
    </section>

    <section class="widget">
        <h3 class="widget-title"><a href='/tags/'>标签</a></h3>
<div class="tagcloud">
    
    <a href="https://code0xff.org/tags/ai/">AI </a>
    
    <a href="https://code0xff.org/tags/ambari/">Ambari </a>
    
    <a href="https://code0xff.org/tags/architecture/">architecture </a>
    
    <a href="https://code0xff.org/tags/bigdata/">bigdata </a>
    
    <a href="https://code0xff.org/tags/btrace/">BTrace </a>
    
    <a href="https://code0xff.org/tags/b%E6%A0%91/">B树 </a>
    
    <a href="https://code0xff.org/tags/c&#43;&#43;/">C&#43;&#43; </a>
    
    <a href="https://code0xff.org/tags/calcite/">calcite </a>
    
    <a href="https://code0xff.org/tags/cmu-database/">CMU-Database </a>
    
    <a href="https://code0xff.org/tags/data_ingestion/">Data_Ingestion </a>
    
    <a href="https://code0xff.org/tags/deltalake/">DeltaLake </a>
    
    <a href="https://code0xff.org/tags/doris/">Doris </a>
    
    <a href="https://code0xff.org/tags/english/">English </a>
    
    <a href="https://code0xff.org/tags/es/">ES </a>
    
    <a href="https://code0xff.org/tags/facebook/">Facebook </a>
    
    <a href="https://code0xff.org/tags/flink/">flink </a>
    
    <a href="https://code0xff.org/tags/flume/">flume </a>
    
    <a href="https://code0xff.org/tags/gc/">GC </a>
    
    <a href="https://code0xff.org/tags/gluten/">Gluten </a>
    
    <a href="https://code0xff.org/tags/hana/">HANA </a>
    
    <a href="https://code0xff.org/tags/hive/">Hive </a>
    
    <a href="https://code0xff.org/tags/iceberg/">iceberg </a>
    
    <a href="https://code0xff.org/tags/impala/">Impala </a>
    
    <a href="https://code0xff.org/tags/janino/">janino </a>
    
    <a href="https://code0xff.org/tags/k8s/">k8s </a>
    
    <a href="https://code0xff.org/tags/kafka/">Kafka </a>
    
    <a href="https://code0xff.org/tags/kudu/">kudu </a>
    
    <a href="https://code0xff.org/tags/kyuubi/">Kyuubi </a>
    
    <a href="https://code0xff.org/tags/lakehouse/">Lakehouse </a>
    
    <a href="https://code0xff.org/tags/leveldb/">LevelDB </a>
    
    <a href="https://code0xff.org/tags/llamafactory/">LlamaFactory </a>
    
    <a href="https://code0xff.org/tags/llvm/">LLVM </a>
    
    <a href="https://code0xff.org/tags/log4j/">log4j </a>
    
    <a href="https://code0xff.org/tags/manacher/">Manacher </a>
    
    <a href="https://code0xff.org/tags/mapreduce/">MapReduce </a>
    
    <a href="https://code0xff.org/tags/micro-service/">micro-service </a>
    
    <a href="https://code0xff.org/tags/mysql/">MySQL </a>
    
    <a href="https://code0xff.org/tags/newsql/">NewSQL </a>
    
    <a href="https://code0xff.org/tags/oceanbase/">OceanBase </a>
    
    <a href="https://code0xff.org/tags/openlogreplicator/">OpenLogReplicator </a>
    
    <a href="https://code0xff.org/tags/parquet/">parquet </a>
    
    <a href="https://code0xff.org/tags/paxos/">paxos </a>
    
    <a href="https://code0xff.org/tags/presto/">Presto </a>
    
    <a href="https://code0xff.org/tags/quick-sql/">quick-sql </a>
    
    <a href="https://code0xff.org/tags/raft/">raft </a>
    
    <a href="https://code0xff.org/tags/scala/">scala </a>
    
    <a href="https://code0xff.org/tags/simd/">SIMD </a>
    
    <a href="https://code0xff.org/tags/snowflake/">snowflake </a>
    
    <a href="https://code0xff.org/tags/spark/">spark </a>
    
    <a href="https://code0xff.org/tags/sre/">SRE </a>
    
    <a href="https://code0xff.org/tags/teradata/">TeraData </a>
    
    <a href="https://code0xff.org/tags/tpcx-hs/">TPCx-HS </a>
    
    <a href="https://code0xff.org/tags/trino/">Trino </a>
    
    <a href="https://code0xff.org/tags/tuning/">Tuning </a>
    
    <a href="https://code0xff.org/tags/unix/">unix </a>
    
    <a href="https://code0xff.org/tags/yarn/">YARN </a>
    
    <a href="https://code0xff.org/tags/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/">二分查找 </a>
    
    <a href="https://code0xff.org/tags/%E4%BA%8C%E5%8F%89%E6%A0%91%E9%81%8D%E5%8E%86/">二叉树遍历 </a>
    
    <a href="https://code0xff.org/tags/%E5%88%86%E5%B8%83%E5%BC%8F/">分布式 </a>
    
    <a href="https://code0xff.org/tags/%E5%88%97%E5%AD%98/">列存 </a>
    
    <a href="https://code0xff.org/tags/%E5%8A%A8%E6%80%81%E6%B3%A8%E5%85%A5/">动态注入 </a>
    
    <a href="https://code0xff.org/tags/%E5%8E%86%E5%8F%B2/">历史 </a>
    
    <a href="https://code0xff.org/tags/%E5%90%91%E9%87%8F%E5%8C%96/">向量化 </a>
    
    <a href="https://code0xff.org/tags/%E5%9B%9E%E6%BA%AF/">回溯 </a>
    
    <a href="https://code0xff.org/tags/%E5%9B%BD%E5%86%85%E6%97%85%E8%A1%8C/">国内旅行 </a>
    
    <a href="https://code0xff.org/tags/%E5%9B%BD%E5%A4%96%E6%97%85%E8%A1%8C/">国外旅行 </a>
    
    <a href="https://code0xff.org/tags/%E5%A4%9A%E7%A7%9F%E6%88%B7/">多租户 </a>
    
    <a href="https://code0xff.org/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据 </a>
    
    <a href="https://code0xff.org/tags/%E5%AD%98%E5%82%A8/">存储 </a>
    
    <a href="https://code0xff.org/tags/%E5%B7%A5%E4%BD%9C%E8%AE%B0%E5%BD%95/">工作记录 </a>
    
    <a href="https://code0xff.org/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/">微服务 </a>
    
    <a href="https://code0xff.org/tags/%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8F%B0/">数据中台 </a>
    
    <a href="https://code0xff.org/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/">数据库 </a>
    
    <a href="https://code0xff.org/tags/%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B/">数据模型 </a>
    
    <a href="https://code0xff.org/tags/%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB/">数据迁移 </a>
    
    <a href="https://code0xff.org/tags/%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96/">查询优化 </a>
    
    <a href="https://code0xff.org/tags/%E6%9F%A5%E8%AF%A2%E7%BC%96%E8%AF%91/">查询编译 </a>
    
    <a href="https://code0xff.org/tags/%E6%A6%82%E7%8E%87/">概率 </a>
    
    <a href="https://code0xff.org/tags/%E6%B1%87%E7%BC%96/">汇编 </a>
    
    <a href="https://code0xff.org/tags/%E6%B5%8B%E8%AF%95/">测试 </a>
    
    <a href="https://code0xff.org/tags/%E7%94%9F%E6%B4%BB/">生活 </a>
    
    <a href="https://code0xff.org/tags/%E7%94%B5%E5%BD%B1/">电影 </a>
    
    <a href="https://code0xff.org/tags/%E7%AE%97%E6%B3%95/">算法 </a>
    
    <a href="https://code0xff.org/tags/%E7%B4%A2%E5%BC%95/">索引 </a>
    
    <a href="https://code0xff.org/tags/%E7%BB%8F%E6%B5%8E/">经济 </a>
    
    <a href="https://code0xff.org/tags/%E7%BC%93%E5%AD%98/">缓存 </a>
    
    <a href="https://code0xff.org/tags/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/">编译原理 </a>
    
    <a href="https://code0xff.org/tags/%E7%BD%91%E7%BB%9C/">网络 </a>
    
    <a href="https://code0xff.org/tags/%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6/">计算框架 </a>
    
    <a href="https://code0xff.org/tags/%E8%AE%BA%E6%96%87/">论文 </a>
    
    <a href="https://code0xff.org/tags/%E8%AF%BB%E4%B9%A6/">读书 </a>
    
    <a href="https://code0xff.org/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记 </a>
    
</div>
    </section>

    

    <section class="widget">
        <h3 class="widget-title">RSS</h3>
        <ul class="widget-list">
            <li><a href="https://code0xff.org/index.xml">文章 RSS</a></li>
        </ul>
    </section>
</div>
            </div>
        </div>
    </div>
    <footer id="footer">
    <div>
        &copy; 2025 <a href="https://code0xff.org/">记录每个瞬间 By 老王</a>
        
    </div>
    <br />
    <div>
        <div class="github-badge">
            <a href="https://gohugo.io/" target="_black" rel="nofollow"><span class="badge-subject">Powered by</span><span class="badge-value bg-blue">Hugo</span></a>
        </div>
        <div class="github-badge">
            <a href="https://github.com/flysnow-org/maupassant-hugo" target="_black"><span class="badge-subject">Theme</span><span class="badge-value bg-yellowgreen">Maupassant</span></a>
        </div>
    </div>
</footer>


    
    <script type="text/javascript">
        window.MathJax = {
            tex2jax: {
                inlineMath: [['$', '$']],
                processEscapes: true
                }
            };
    </script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script><script src="https://cdn.bootcdn.net/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js"></script>

<a id="rocket" href="#top"></a>
<script type="text/javascript" src='/js/totop.js?v=0.0.0' async=""></script>

<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'GA ID', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>



    <script type="text/javascript" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script>




</body>

</html>