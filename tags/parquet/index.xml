<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>parquet on 记录每个瞬间</title>
    <link>https://code0xff.org/tags/parquet/</link>
    <description>Recent content in parquet on 记录每个瞬间</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Tue, 22 Oct 2024 09:08:07 +0800</lastBuildDate><atom:link href="https://code0xff.org/tags/parquet/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>DataX和Canal相关</title>
      <link>https://code0xff.org/post/2024/10/datax_canal/</link>
      <pubDate>Tue, 22 Oct 2024 09:08:07 +0800</pubDate>
      
      <guid>https://code0xff.org/post/2024/10/datax_canal/</guid>
      <description>DataX：读写插件，Job任务拆分，Task和Task Group，transform(filter，substr，replace，可自定义)，流控，脏数据，数据库冥等写入，ETL架构。Canal：Server（服务端-客户端模式，嵌入式模式），多个instance，包括：eventParser (数据源接入，模拟slave协议和master进行交互，协议解析)、eventSink (Parser和Store链接器，进行数据过滤，加工，分发的工作)、eventStore (数据存储)、metaManager (增量订阅&amp;amp;消费信息管理器)</description>
    </item>
    
    <item>
      <title>Parquet for Spark</title>
      <link>https://code0xff.org/post/2024/10/parquet_4_spark/</link>
      <pubDate>Wed, 02 Oct 2024 09:08:07 +0800</pubDate>
      
      <guid>https://code0xff.org/post/2024/10/parquet_4_spark/</guid>
      <description>Spark执行Delta的过程，通过自定义的format格式，到DataFrameWriter.saveToV1Source，在是到DeltaDataSource#createRealation，写入做优化事务处理，再用FileFormatWriter创建多个Task并行写入，之后就是到Parquet内部执行阶段。Parquet包含Row Groups，往下是Column Chunk，再往下是Page，文件尾部包含Footer和一些元数据信息。Spark是按行写入的，一次写一行，每行写对应的 column。Parquet编码包括Dictionary Encoding、Run Length Encoding (RLE)、Delta Encoding。读取的主要类是VectorizedParquetRecordReader执行一批读取，调用VectorizedColumnReaders(对应每个column)，再调用VectorizedValuesReader(读取一个column中的一段数据)，返回由上层应用消费 。</description>
    </item>
    
  </channel>
</rss>
